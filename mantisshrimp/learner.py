# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/09_learner.ipynb (unless otherwise specified).

__all__ = ['Learner']

# Cell
from .imports import *
from .core import *
from .models import *

# Cell
class Learner:
    def __init__(self, m, train_dl, valid_dl, opt_fn, logger=None):
        store_attr(self, 'm,train_dl,valid_dl,opt_fn')
        self.logger = logger or True
        self.gpus = get_all_available_gpus()

    @delegates(Trainer.__init__)
    def fit(self, max_epochs, lr, lr_sched=None, gpus=None, callbacks=None, **kwargs):
        self.m.configure_optimizers = self._configure_optimizers(lr, lr_sched)
        gpus = ifnone(gpus, self.gpus)
        cbs = L(LearningRateLogger()) + L(callbacks)
        trainer = Trainer(max_epochs=max_epochs, logger=self.logger, callbacks=cbs,  gpus=gpus, **kwargs)
        trainer.fit(self.m, self.train_dl, self.valid_dl)

    @delegates(Trainer.__init__)
    def fit_one_cycle(self, max_epochs, lr_max, pct_start=.25, **kwargs):
        def lr_sched(opt):
            sched = OneCycleLR(opt, lr_max, len(self.train_dl)*max_epochs, pct_start=pct_start)
            return {'scheduler':sched, 'interval':'step'}
        return self.fit(max_epochs=max_epochs, lr=lr_max, lr_sched=lr_sched, **kwargs)

    @delegates(Trainer.__init__)
    def lr_find(self, gpus=None, **kwargs):
        self.m.configure_optimizers = self._configure_optimizers(0, None)
        gpus = ifnone(gpus, self.gpus)
        return Trainer(gpus=gpus, **kwargs).lr_find(self.m, self.train_dl, self.valid_dl)

    def _configure_optimizers(self, lr, sched_fn=None):
        opt = self.opt_fn(params(self.m), lr)
        def _inner(self):
            if notnone(sched_fn): return [opt], [sched_fn(opt)]
            return opt
        return MethodType(_inner, self.m)